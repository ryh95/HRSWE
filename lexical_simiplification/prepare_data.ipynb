{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "Download [lex.mturk](https://cs.pomona.edu/~dkauchak/simplification/lex.mturk.14/lex.mturk.14.tar.gz)\n",
    " and extract sentences, targets, and candidates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('task_data/lex.mturk.txt','r',encoding='iso-8859-1') as f:\n",
    "     # open('task_data/lex_mturk_sen.txt','w') as f_sen,\\\n",
    "     # open('task_data/targets.pickle','wb') as f_targets,\\\n",
    "     # open('task_data/candidates.pickle','wb') as f_candidates:\n",
    "     # open('task_data/pos_tags.pickle','wb') as f_pos_tags:\n",
    "    f_str = f.readline()\n",
    "    sens,targets,candidates,pos_tags = [],[],[],[]\n",
    "    for l in f:\n",
    "        sen,target,*candidate = l.strip().split('\\t')\n",
    "        # sen_pos = nltk.pos_tag(sen.split())\n",
    "        # id = [w for w,tag in sen_pos].index(target)\n",
    "        # target_pos = [tag for w,tag in sen_pos][id]\n",
    "        # f_sen.write(sen+'\\n')\n",
    "        # pos_tags.append(target_pos)\n",
    "        sens.append(sen.strip('\"').split())\n",
    "        targets.append(target)\n",
    "        candidates.append(set(candidate))\n",
    "    # pickle.dump(targets,f_targets,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    # pickle.dump(candidates,f_candidates,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    # pickle.dump(pos_tags,f_pos_tags,pickle.HIGHEST_PROTOCOL)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "check whether targets in the sentences"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i,(target,sen) in enumerate(zip(targets,sens)):\n",
    "    if target not in sen:\n",
    "        print(sen)\n",
    "        print(targets)\n",
    "        print(i)\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "correct the sentences"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sens[111][0] = sens[111][0].lower()\n",
    "sens[187][0] = sens[187][0].lower()\n",
    "sens[209][0] = sens[209][0].lower()\n",
    "sens[301][0] = sens[301][0].lower()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "obtain the pos tags of targets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import stanza"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "2020-06-02 16:28:56 INFO: Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ewt     |\n",
      "| pos       | ewt     |\n",
      "=======================\n",
      "\n",
      "2020-06-02 16:28:56 INFO: Use device: gpu\n",
      "2020-06-02 16:28:56 INFO: Loading: tokenize\n",
      "2020-06-02 16:28:56 INFO: Loading: pos\n",
      "2020-06-02 16:28:59 INFO: Done loading processors!\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# prepare pos tags of target words\n",
    "nlp = stanza.Pipeline('en',processors='tokenize,pos',tokenize_pretokenized=True)\n",
    "doc = nlp(sens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "pos_tags = []\n",
    "for doc_sentence,target,sen in zip(doc.sentences,targets,sens):\n",
    "    id = sen.index(target)\n",
    "    word = doc_sentence.words[id]\n",
    "    pos_tags.append(word.xpos)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "check whether candidates have phrases"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i,candidate in enumerate(candidates):\n",
    "    for c in candidate:\n",
    "        if len(c.split()) >= 2:\n",
    "            print(candidate)\n",
    "            print(i)\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "remove phrases since\n",
    "\n",
    "1. word2vec only contains tokens\n",
    "2. words in constrain are not phrases"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "w_candidates = []\n",
    "for candidate in candidates:\n",
    "    # only preserve tokens that are not phrases\n",
    "    w_candidates.append({c for c in candidate if len(c.split()) == 1})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "chech whether candidate words have spaces in the end of the word\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for candidate in w_candidates:\n",
    "    for c in candidate:\n",
    "        if c[-1] == ' ':\n",
    "            print(c)\n",
    "            print(candidate)\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "remove the spaces\n",
    " "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "w_candidates_ = []\n",
    "for candidate in w_candidates:\n",
    "    w_candidates_.append({c.strip(' ') for c in candidate})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "lemmatize the target and the candidates"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "from utils import penn2morphy, get_all_words_covered_in_constraint, generate_sub_thesauri\n",
    "lemmatizer = WordNetLemmatizer()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "lemma_targets,lemma_candidates = [],[]\n",
    "for target,cand_set,tag in zip(targets,w_candidates_,pos_tags):\n",
    "    tag_ = penn2morphy(tag)\n",
    "    lemma_cand_set = set()\n",
    "    lemma_targets.append(lemmatizer.lemmatize(target,tag_)) \n",
    "    for c in cand_set:\n",
    "        lemma_cand_set.add(lemmatizer.lemmatize(c,tag_))\n",
    "    lemma_candidates.append(lemma_cand_set)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "check the candidates coverage"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "set_candidates = set()\n",
    "for candidate in lemma_candidates:\n",
    "    set_candidates |= candidate"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "2616"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 14
    }
   ],
   "source": [
    "len(set_candidates)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "constrain_vocab = get_all_words_covered_in_constraint(['synonyms.txt','antonyms.txt'],set_candidates)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "2310"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 13
    }
   ],
   "source": [
    "len(set_candidates & constrain_vocab),len(set_candidates)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "check the targets & candidates coverage"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "set_targets_candidates =  set(lemma_targets) | set_candidates"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "constrain_vocab = get_all_words_covered_in_constraint(['synonyms.txt','antonyms.txt'],set_targets_candidates)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "(2549, 2848)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 16
    }
   ],
   "source": [
    "len(set_targets_candidates & constrain_vocab),len(set_targets_candidates)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "(432, 441)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 37
    }
   ],
   "source": [
    "len(set(lemma_targets) & constrain_vocab),len(set(lemma_targets)) "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "(2325, 2616)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 38
    }
   ],
   "source": [
    "len(set_candidates & constrain_vocab),len(set(set_candidates)) "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "investigate that words are not covered"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "rest_vocab = set_targets_candidates - (set_targets_candidates & constrain_vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "['sepcial', 'noticable']"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 36
    }
   ],
   "source": [
    "random.sample(rest_vocab,2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "replace the target in sentences with lemma_targets "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for sen,target,lemma_target in zip(sens,targets,lemma_targets):\n",
    "    id = sen.index(target)\n",
    "    sen[id] = lemma_target"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "now prepare the final vocab, constrain_vocab, and embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for sen in sens:\n",
    "    vocab.update(sen)\n",
    "vocab |= set_targets_candidates"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "6278"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 17
    }
   ],
   "source": [
    "len(vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "from preprocess import GeneralTextProcesser\n",
    "from constants import ORIGINAL_VECS_DIR, ORIGINAL_EMBEDDING, THESAURUS_DIR"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "preparing vocab vectors\n",
      "6278 words in vocab, 514 words not found in word embedding file, ignore them in the embedding\n",
      "saving vocab vector file\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "l_vocab = list(vocab)\n",
    "text_preprocesser = GeneralTextProcesser()\n",
    "emb_dict,_ = text_preprocesser.vocab2vec(l_vocab, ORIGINAL_VECS_DIR, 'lexical_simplification', ORIGINAL_EMBEDDING,\n",
    "                                       ['pickle'], 'word2vec', normalize=False, oov_handle='none')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "final_vocab = set(emb_dict.keys())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "5764"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 22
    }
   ],
   "source": [
    "len(final_vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "constrain_vocab = get_all_words_covered_in_constraint(['synonyms.txt','antonyms.txt'],final_vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# save constrain emb dict\n",
    "constrain_emb_dict = {w:emb_dict[w] for w in constrain_vocab}\n",
    "with open('lexical_simplification_constrain.pickle','wb') as f:\n",
    "    pickle.dump(constrain_emb_dict,f,pickle.HIGHEST_PROTOCOL)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os.path import join"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# save vocab\n",
    "np.save('lexical_simplification.npy',list(final_vocab))\n",
    "np.save('lexical_simplification_constrain.npy',list(constrain_vocab))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "generate thesauri "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generate_sub_thesauri(join(THESAURUS_DIR, 'synonyms.txt'),'synonyms.txt',final_vocab)\n",
    "generate_sub_thesauri(join(THESAURUS_DIR, 'antonyms.txt'),'antonyms.txt',final_vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "save sentences, targets, candidates, and pos tags "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# type(sens),type(lemma_targets),type(lemma_candidates),type(pos_tags)\n",
    "with open('lex_mturk_sen.pickle','wb') as f:\n",
    "    pickle.dump(sens,f,pickle.HIGHEST_PROTOCOL)\n",
    "with open('targets.pickle','wb') as f:\n",
    "    pickle.dump(lemma_targets,f,pickle.HIGHEST_PROTOCOL)\n",
    "with open('candidates.pickle','wb') as f:\n",
    "    pickle.dump(lemma_candidates,f,pickle.HIGHEST_PROTOCOL)\n",
    "with open('pos_tags.pickle','wb') as f:\n",
    "    pickle.dump(pos_tags,f,pickle.HIGHEST_PROTOCOL)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}